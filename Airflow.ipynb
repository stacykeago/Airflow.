{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stacykeago/Airflow./blob/main/Airflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ_wggORLsxz"
      },
      "outputs": [],
      "source": [
        "# Define the method\n",
        "def pull_file(URL, savepath):\n",
        "    r = requests.get(URL)\n",
        "    with open(savepath, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    # Use the print method for logging\n",
        "    print(f\"File pulled from {URL} and saved to {savepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the PythonOperator class\n",
        "from airflow.operators.python import PythonOperator"
      ],
      "metadata": {
        "id": "WD5UD7meMq1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the task\n",
        "pull_file_task = PythonOperator(task_id='pull_file',\n",
        "    # Add the callable\n",
        "    python_callable=pull_file,\n",
        "    # Define the arguments\n",
        "    op_kwargs={'URL': 'http://dataserver/sales.json', 'savepath': 'latestsales.json'}\n",
        ")"
      ],
      "metadata": {
        "id": "uAJiPtWyUZ0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add another Python task\n",
        "parse_file_task = PythonOperator(\n",
        "    task_id= 'parse_file',\n",
        "    # Set the function to call\n",
        "      python_callable=parse_file,\n",
        "    # Add the arguments\n",
        "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
        ")\n"
      ],
      "metadata": {
        "id": "nvhSOy3MVwTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Operator\n",
        "from airflow.operators.email import EmailOperator\n",
        "\n",
        "# Define the task\n",
        "email_manager_task = EmailOperator(\n",
        "    task_id='email_manager',\n",
        "    to='manager@datacamp.com',\n",
        "    subject='Latest sales JSON',\n",
        "    html_content='Attached is the latest sales JSON file as requested.',\n",
        "    files='parsedfile.json',\n",
        "    dag=process_sales_dag\n",
        ")\n",
        "\n",
        "# Set the order of tasks\n",
        "pull_file_task >> parse_file_task >> email_manager_task"
      ],
      "metadata": {
        "id": "EePpXtAlXFxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the scheduling arguments as defined\n",
        "default_args = {\n",
        "  'owner': 'Engineering',\n",
        "  'start_date': datetime(2023, 11, 1),\n",
        "  'email': ['airflowresults@datacamp.com'],\n",
        "  'email_on_failure': False,\n",
        "  'email_on_retry': False,\n",
        "  'retries': 3,\n",
        "  'retry_delay': timedelta(minutes=20)\n",
        "}\n",
        "\n",
        "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
      ],
      "metadata": {
        "id": "JPMuI2eZZTBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.sensors.filesystem import FileSensor\n",
        "from datetime import datetime\n",
        "\n",
        "report_dag = DAG(\n",
        "    dag_id = 'execute_report',\n",
        "    schedule_interval = \"0 0 * * *\"\n",
        ")\n",
        "\n",
        "precheck = FileSensor(\n",
        "    task_id='check_for_datafile',\n",
        "    filepath='salesdata_ready.csv',\n",
        "    start_date=datetime(2024,1,20),\n",
        "    mode='poke',\n",
        "    dag=report_dag\n",
        ")\n",
        "\n",
        "generate_report_task = BashOperator(\n",
        "    task_id='generate_report',\n",
        "    bash_command='generate_report.sh',\n",
        "    start_date=datetime(2024,1,20),\n",
        "    dag=report_dag\n",
        ")\n",
        "\n",
        "precheck >> generate_report_task\n"
      ],
      "metadata": {
        "id": "rK_6ow3p0E5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "sample_dag = DAG(\n",
        "    dag_id = 'sample_dag',\n",
        "    schedule_interval = \"0 0 * * *\"\n",
        ")\n",
        "\n",
        "sample_task = BashOperator(\n",
        "    task_id='sample',\n",
        "    bash_command='generate_sample.sh',\n",
        "    start_date=datetime(2023,2,20),\n",
        "    dag=sample_dag\n",
        ")\n"
      ],
      "metadata": {
        "id": "CtOL2db5_KWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the timedelta object\n",
        "from datetime import timedelta\n",
        "\n",
        "# Create the dictionary entry\n",
        "default_args = {\n",
        "  'start_date': datetime(2024, 1, 20),\n",
        "  'sla': timedelta(minutes=30)\n",
        "}\n",
        "\n",
        "# Add to the DAG\n",
        "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval=None)"
      ],
      "metadata": {
        "id": "37k7G3VpBRae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the timedelta object\n",
        "from datetime import timedelta\n",
        "\n",
        "test_dag = DAG('test_workflow', start_date=datetime(2024,1,20), schedule_interval=None)\n",
        "\n",
        "# Create the task with the SLA\n",
        "task1 = BashOperator(task_id='first_task',\n",
        "                     sla=timedelta(hours=3),\n",
        "                     bash_command='initialize_data.sh',\n",
        "                     dag=test_dag)"
      ],
      "metadata": {
        "id": "fWjOHXJjCn9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the email task\n",
        "email_report = EmailOperator(\n",
        "        task_id='email_report',\n",
        "        to='airflow@datacamp.com',\n",
        "        subject='Airflow Monthly Report',\n",
        "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
        "        files=['monthly_report.pdf'],\n",
        "        dag=report_dag\n",
        ")\n",
        "\n",
        "# Set the email task to run after the report is generated\n",
        "generate_report >> email_report"
      ],
      "metadata": {
        "id": "5xxsw3QbEY0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.sensors.filesystem import FileSensor\n",
        "from datetime import datetime\n",
        "\n",
        "default_args={\n",
        "    '____': [____],\n",
        "    '____': True,\n",
        "    ____\n",
        "}\n",
        "report_dag = DAG(\n",
        "    dag_id = 'execute_report',\n",
        "    schedule_interval = \"0 0 * * *\",\n",
        "    default_args=default_args\n",
        ")\n",
        "\n",
        "precheck = FileSensor(\n",
        "    task_id='check_for_datafile',\n",
        "    filepath='salesdata_ready.csv',\n",
        "    start_date=datetime(2023,2,20),\n",
        "    mode='reschedule',\n",
        "    dag=report_dag)\n",
        "\n",
        "generate_report_task = BashOperator(\n",
        "    task_id='generate_report',\n",
        "    bash_command='generate_report.sh',\n",
        "    start_date=datetime(2023,2,20),\n",
        "    dag=report_dag\n",
        ")\n",
        "\n",
        "precheck >> generate_report_task\n"
      ],
      "metadata": {
        "id": "iW33ypFcE9wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "  'start_date': datetime(2023, 4, 15),\n",
        "}\n",
        "\n",
        "cleandata_dag = DAG('cleandata',\n",
        "                    default_args=default_args,\n",
        "                    schedule_interval='@daily')\n",
        "\n",
        "# Create a templated command to execute\n",
        "# 'bash cleandata.sh datestring'\n",
        "templated_command = 'bash cleandata.sh {{ ds_nodash }}'  # {{ ds_nodash }} will be replaced by the execution date without dashes\n",
        "\n",
        "# Modify clean_task to use the templated command\n",
        "clean_task = BashOperator(task_id='cleandata_task',\n",
        "                          bash_command=templated_command,  # Use the templated command\n",
        "                          dag=cleandata_dag)"
      ],
      "metadata": {
        "id": "lFEaYcA9IvBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "    'start_date': datetime(2023, 4, 15),\n",
        "}\n",
        "\n",
        "cleandata_dag = DAG('cleandata',\n",
        "                    default_args=default_args,\n",
        "                    schedule_interval='@daily')\n",
        "\n",
        "# Modify the templated command to handle a second argument called filename\n",
        "templated_command = \"\"\"\n",
        "  bash cleandata.sh {{ ds_nodash }} {{ params.filename }}\n",
        "\"\"\"\n",
        "\n",
        "# Modify clean_task to pass the new argument\n",
        "clean_task = BashOperator(task_id='cleandata_task',\n",
        "                          bash_command=templated_command,\n",
        "                          params={'filename': 'salesdata.txt'},  # Specify the filename\n",
        "                          dag=cleandata_dag)\n",
        "\n",
        "# Create a new BashOperator clean_task2\n",
        "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
        "                           bash_command=templated_command,\n",
        "                           params={'filename': 'supportdata.txt'},  # Specify the filename\n",
        "                           dag=cleandata_dag)\n",
        "\n",
        "# Set the operator dependencies\n",
        "clean_task >> clean_task2\n"
      ],
      "metadata": {
        "id": "XjnU_3QHJHgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "filelist = [f'file{x}.txt' for x in range(30)]\n",
        "\n",
        "default_args = {\n",
        "    'start_date': datetime(2020, 4, 15),\n",
        "}\n",
        "\n",
        "cleandata_dag = DAG('cleandata',\n",
        "                    default_args=default_args,\n",
        "                    schedule_interval='@daily')\n",
        "\n",
        "# Modify the template to handle multiple files in a single run\n",
        "templated_command = \"\"\"\n",
        "  {% for filename in params.filenames %}\n",
        "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
        "  {% endfor %}\n",
        "\"\"\"\n",
        "\n",
        "# Modify clean_task to use the templated command\n",
        "clean_task = BashOperator(task_id='cleandata_task',\n",
        "                          bash_command=templated_command,\n",
        "                          params={'filenames': filelist},  # Pass the list of filenames as params\n",
        "                          dag=cleandata_dag)\n"
      ],
      "metadata": {
        "id": "KpltDjs5Jdkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.email import EmailOperator\n",
        "from airflow.macros import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# Create the string representing the html email content\n",
        "html_email_str = \"\"\"\n",
        "Date: {{ ds }}\n",
        "Username: {{ params.username }}\n",
        "\"\"\"\n",
        "\n",
        "email_dag = DAG('template_email_test',\n",
        "                default_args={'start_date': datetime(2023, 4, 15)},\n",
        "                schedule_interval='@weekly')\n",
        "\n",
        "email_task = EmailOperator(task_id='email_task',\n",
        "                           to='testuser@datacamp.com',\n",
        "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
        "                           html_content=html_email_str,\n",
        "                           params={'username': 'testemailuser'},\n",
        "                           dag=email_dag)\n"
      ],
      "metadata": {
        "id": "31emJRjuJ0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to determine if years are different\n",
        "def year_check(**kwargs):\n",
        "    current_year = int(kwargs['ds_nodash'][0:4])\n",
        "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
        "    if current_year == previous_year:\n",
        "        return 'current_year_task'\n",
        "    else:\n",
        "        return 'new_year_task'\n",
        "\n",
        "# Define the BranchPythonOperator\n",
        "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
        "                                   python_callable=year_check, provide_context=True)\n",
        "# Define the dependencies\n",
        "branch_task >> current_year_task\n",
        "branch_task >> new_year_task"
      ],
      "metadata": {
        "id": "ZNatmbacKKBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.sensors.filesystem import FileSensor\n",
        "\n",
        "# Import the needed operators\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import date, datetime\n",
        "\n",
        "def process_data(**context):\n",
        "    file = open('/home/repl/workspace/processed_data.tmp', 'w')\n",
        "    file.write(f'Data processed on {date.today()}')\n",
        "    file.close()\n",
        "\n",
        "dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2023, 4, 1)})\n",
        "\n",
        "sensor = FileSensor(task_id='sense_file',\n",
        "                    filepath='/home/repl/workspace/startprocess.txt',\n",
        "                    poke_interval=5,\n",
        "                    timeout=15,\n",
        "                    dag=dag)\n",
        "\n",
        "bash_task = BashOperator(task_id='cleanup_tempfiles',\n",
        "                         bash_command='rm -f /home/repl/*.tmp',\n",
        "                         dag=dag)\n",
        "\n",
        "python_task\n"
      ],
      "metadata": {
        "id": "vJ0XOy2qK-hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.sensors.filesystem import FileSensor\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "from dags.process import process_data\n",
        "from datetime import timedelta, datetime\n",
        "\n",
        "# Update the default arguments and apply them to the DAG\n",
        "default_args = {\n",
        "  'start_date': datetime(2023,1,1),\n",
        "  'sla': timedelta(minutes=90)\n",
        "}\n",
        "\n",
        "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
        "\n",
        "sensor = FileSensor(task_id='sense_file',\n",
        "                    filepath='/home/repl/workspace/startprocess.txt',\n",
        "                    poke_interval=45,\n",
        "                    dag=dag)\n",
        "\n",
        "bash_task = BashOperator(task_id='cleanup_tempfiles',\n",
        "                         bash_command='rm -f /home/repl/*.tmp',\n",
        "                         dag=dag)\n",
        "\n",
        "python_task = PythonOperator(task_id='run_processing',\n",
        "                             python_callable=process_data,\n",
        "                             provide_context=True,\n",
        "                             dag=dag)\n",
        "\n",
        "sensor >> bash_task >> python_task\n"
      ],
      "metadata": {
        "id": "xN4PMtzcOHcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.sensors.filesystem import FileSensor\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
        "from airflow.operators.dummy import DummyOperator\n",
        "from airflow.operators.email import EmailOperator\n",
        "from dags.process import process_data\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Update the default arguments and apply them to the DAG.\n",
        "default_args = {\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'sla': timedelta(minutes=90)\n",
        "}\n",
        "\n",
        "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
        "\n",
        "sensor = FileSensor(task_id='sense_file',\n",
        "                    filepath='/home/repl/workspace/startprocess.txt',\n",
        "                    poke_interval=45,\n",
        "                    dag=dag)\n",
        "\n",
        "bash_task = BashOperator(task_id='cleanup_tempfiles',\n",
        "                         bash_command='rm -f /home/repl/*.tmp',\n",
        "                         dag=dag)\n",
        "\n",
        "python_task = PythonOperator(task_id='run_processing',\n",
        "                             python_callable=process_data,\n",
        "                             provide_context=True,\n",
        "                             dag=dag)\n",
        "\n",
        "email_subject = \"\"\"\n",
        "    Email report for {{ params.department }} on {{ ds_nodash }}\n",
        "\"\"\"\n",
        "\n",
        "email_report_task = EmailOperator(task_id='email_report_task',\n",
        "                                  to='sales@mycompany.com',\n",
        "                                  subject=email_subject,\n",
        "                                  html_content='',\n",
        "                                  params={'department': 'Data subscription services'},\n",
        "                                  dag=dag)\n",
        "\n",
        "no_email_task = DummyOperator(task_id='no_email_task', dag=dag)\n",
        "\n",
        "def check_weekend(**kwargs):\n",
        "    dt = datetime.strptime(kwargs['execution_date'], \"%Y-%m-%d\")\n",
        "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
        "    if (dt.weekday() < 5):\n",
        "        return 'email_report_task'\n",
        "    else:\n",
        "        return 'no_email_task'\n",
        "\n",
        "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
        "                                   python_callable=check_weekend,\n",
        "                                   provide_context=True,\n",
        "                                   dag=dag)\n",
        "\n",
        "sensor >> bash_task >> python_task\n",
        "\n",
        "python_task >> branch_task >> [email_report_task, no_email_task]\n"
      ],
      "metadata": {
        "id": "gen8sOL7Oeyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}